package application;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.util.*;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.io.Writer;

import edu.stanford.nlp.parser.lexparser.LexicalizedParser;

import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;

/**
 * The processor is the top level controller of the whole project and
 * it has several functionalities as below: 
 * Step 1. Read in files/documents
 * Step 2. The processor goes into one of the two modes below
 * 		>> Labeling mode. The processor labels the samples by utilizing set of rules
 * 		   then output the result (feature + unsupervised label) to file for training  
 *      >> exporting to DB mode. The processor labels samples based on the predicted
 *         label generated by ML model. We pre-store the predicted label files.
 */

public class Processor {
	 
	private static final String POS_SEQUENCE_FILE_NAME = "pos_sequence.txt";
	private static final String PREDICTED_LABEL_FILE_NAME_PREFIX = "corpus_label/label";
	private String inputFileName;
	private String outputFileName;

	public static boolean enableExport = true;
	
	/* construct list of sentence object after parsing the JSON file */
	private ArrayList<Article> articles;
	
	/* construct list of relation object as samples */
	private ArrayList<Relation> relations;
	
	/* statistic used, get number of pos and neg samples seperately */
	private int numPositive, numNegative;
	
	/* store the max column index in the traning matrix */
	private int maxColIndex = 0;
	
	/* hacky code, used in chunk running */
	private int startArticle = 5;
	private int endArticle = 10;
	
	/* store word dictionary */
	public static HashMap<String, Integer> dictionary;
	/* store stop word dictionary */
	public static HashMap<String, Integer> stopWordDictionary;
	/* store part-of-speech dictionary */
	public static HashMap<String, Integer> POSDictionary;
	/* store part-of-speech sequence dictionary */
	public static HashMap<String, Integer> POSSequenceDictonary;
	/* store ner dictionary */
	public static HashMap<String, Integer> nerDictionary;
	/* store POS tagger for feature extraction */
	public static Tagger tagger;
	/* store NER for name entity extraction */
	public static NER ner;
	
	/* used in exporting mode */
	public static ArrayList<Integer> predictedLabels;
	public static DBHandler dbHdl;

	
	public Processor(String inputFileName, String outputFileName){
		this.inputFileName = inputFileName;
		this.outputFileName = outputFileName;
		articles = new ArrayList<Article>();
		relations = new ArrayList<Relation>();
		numNegative = 0;
		numPositive = 0;
		dictionary = new HashMap<String, Integer>();
		stopWordDictionary = new HashMap<String, Integer>();
		POSDictionary = new HashMap<String, Integer>();
		POSSequenceDictonary = new HashMap<String, Integer>();
		nerDictionary = new HashMap<String, Integer>();
		tagger = new Tagger();
		ner = new NER();
		
		// initialize db handler in export mode
		if (enableExport) {
			dbHdl = new DBHandler();
		}
	}

	/**
	 * Parse the input JSON file, construct sentence object from each JSON
	 * object for further process and populate the sentence list
	 * @param inf
	 */
	private void parseJSONFile(File inf) {
		/* convert content of the file into string */
		try {
			BufferedReader reader;
			reader = new BufferedReader(new FileReader(inf));

			String line = null;
			StringBuilder  stringBuilder = new StringBuilder();

			while( ( line = reader.readLine() ) != null ) {
				stringBuilder.append( line );
			}
			
			// Parse the JSON string to JSON array
			JSONArray records = new JSONArray(stringBuilder.toString());
			// construct one article object for each JSON object in JSON array
			for (int i = 0; i < records.length(); ++i) {
				JSONObject record = records.getJSONObject(i);
				Article article = new Article(record.getString("url"), 
						record.getString("body"));
				articles.add(article);
			}			
			// close input stream
			reader.close();
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (IOException e1) {
			e1.printStackTrace();
		} catch (JSONException e2) {
			e2.printStackTrace();
		}
	}
	
	/**
	 * Read in necessary dictionary for feature construction
	 * say, words dic., stop word dic., pos dic., ner dic.
	 */
	private void readDictionaries() {
		this.readDictionary(dictionary, "3esl.txt");
		this.readDictionary(stopWordDictionary, "stopword.txt");
		this.readDictionary(POSDictionary, "pos.txt");
		this.readDictionary(nerDictionary, "ner.txt");
//		this.readDictionary(POSSequenceDictonary, "pos_sequence.txt");
	}
	
	/**
	 * After map construction, we will prinf feature index that
	 * denotes the start index of each feature.
	 */
	private String printFeatureIndex() {
		StringBuffer sb = new StringBuffer();
		sb.append("\tword index: " + Feature.WORD_INDEX + "\n");
		sb.append("\tnum words index: " + Feature.NUM_WORDS_INDEX + "\n");
		sb.append("\tnum stop words index: " + Feature.NUM_STOP_WORDS_INDEX + "\n");
		sb.append("\tnum cap words index: " + Feature.NUM_CAP_WORDS_INDEX + "\n");
		sb.append("\tnum punc index: " + Feature.NUM_PUNC_INDEX + "\n");
		sb.append("\tnum nps btw: " + Feature.NUM_NPS_BTW + "\n");
		sb.append("\tentity e1 index: " + Feature.ENTITY_E1_INDEX + "\n");
		sb.append("\tentity e2 index: " + Feature.ENTITY_E2_INDEX + "\n");
		sb.append("\tPOS left e1 index: " + Feature.POS_LEFT_E1_INDEX + "\n");
		sb.append("\tPOS right e2 index: " + Feature.POS_RIGHT_E2_INDEX + "\n");
		sb.append("\tlast column (max dimension): " + (Feature.POS_SEQUENCE_INDEX - 1)+ "\n");
		return sb.toString();
	}
	
	/**
	 * Build word dictionary for feature construction for a particular file
	 */
	public void readDictionary(HashMap<String, Integer> dictionary, String fileName) {
		File inf = Processor.readFile(fileName);
		int wordIndex = 0;
		try {
			BufferedReader reader;
			reader = new BufferedReader(new FileReader(inf));

			String line = null;
			while((line = reader.readLine()) != null ) {
				if (dictionary.containsKey(line)) {
					continue;
				} else {
					dictionary.put(line, ++wordIndex);
				}
			}
			reader.close();
		}
		catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (IOException e1) {
			e1.printStackTrace();
		}
	}
	
	/**
	 * Read input file and return File object 
	 * @param inputFileName String input file name
	 * @return File object
	 */
	public static File readFile(String inputFileName) {
		File inf = new File("files/" + inputFileName);
		if(!inf.exists()) { 
			System.err.println("ERROR: File: "+inf.getAbsolutePath()+" does not exist");
			return null;
		}
		if(!inf.canRead()) { 
			System.err.println("ERROR: File: "+inf.getAbsolutePath()+" cannot be read");
			return null;
		}
		
		System.out.println("INFO: finish reading file" + inf.getAbsolutePath());
		return inf;
	}
	
	/**
	 * Extract relations for each article and populate current relations list
	 */
	private void extractRelations() {
		LexicalizedParser lp = 
				LexicalizedParser.loadModel("edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz");
		// run [start_article, end_article) we run the corpus several runs to save time
		for (int i = startArticle; i < endArticle; i++) {
			relations.addAll(articles.get(i).extractRelations(lp));
		}
//		for (Article article : articles) {
//			relations.addAll(article.extractRelations(lp));
//		}
	}
	
	/**
	 * Used in export to db mode, 
	 * export all relations predicated as truth into corresponding tables in db
	 */
	private void exportToDB () {
		// read in predicted label file
		String fileName = PREDICTED_LABEL_FILE_NAME_PREFIX + startArticle + "_" + endArticle + ".txt";
		predictedLabels = constructPredictedLabel(fileName);
		// walk through all relations and save to db
		String url = "", sentence = "";
		int urlid, sentenceid;
		Relation relation;
		ResultSet rs = null;
		for (int i = 0; i < relations.size(); ++i) {
			// throw away sample that predicted as non-relation
			if (predictedLabels.get(i) == 0) 
				continue;
			// insert url
			relation = relations.get(i);
			url = relation.getURL();
			sentence = relation.getSentence();
			try {
				// insert url into table '229url'
				rs = dbHdl.executeQueryCmd("select id from 229url where url = '" + url + "';");
				if (!rs.next()){
					dbHdl.executeUpdateCmd("insert 229url (url) values ('" + relations.get(i).getURL() + "');");
					rs = dbHdl.executeQueryCmd("select id from 229url where url = '" + url + "';");
					rs.next(); urlid = rs.getInt("id");
				} else {
					urlid = rs.getInt("id");
				}

				// insert original sentence where the relation comes from into table '229sentence'
				rs = dbHdl.executeQueryCmd("select id from 229sentence where sentence = '" + sentence + "';");
				if (!rs.next()) {
					dbHdl.executeUpdateCmd("insert 229sentence (sentence, urlid) values ('" + sentence + "'," 
											+ urlid + ");");
					rs = dbHdl.executeQueryCmd("select id from 229sentence where sentence = '" + sentence + "';");
					rs.next(); sentenceid = rs.getInt("id");
				} else {
					sentenceid = rs.getInt("id");
				}
				
				// insert relation (head of e1, verb, head of e2, sentenceid) into table '229corpus'
				// db complains about puns, therefore, we eliminate relation that has verb with puncs in it
				if (relation.getVerb() == null || relation.getHeaderE1() == null || relation.getHeaderE2() == null) 
					continue;
				String query = "select id from 229corpus where e1 = '" + relation.getHeaderE1() + 
						"' and R = '" + relation.getVerb().replaceAll("'", "") + "' and e2 = '" + relation.getHeaderE2() + "';";
				rs = dbHdl.executeQueryCmd(query);
				if (!rs.next()) {
					String update = "insert 229corpus (e1, R, e2, sentenceid) values ('" + relation.getHeaderE1() +
							"','" + relation.getVerb().replace("'", "") + "','" + relation.getHeaderE2() + "'," + sentenceid + ");";
					dbHdl.executeUpdateCmd(update);
				}
			} catch (SQLException e) {
				e.printStackTrace();
			}	

		}
	}
	
	/**
	 * Used in labeling mode
	 * Output samples, output each relation extracted from all articles (features, labels) as a sample
	 * in format of into output file, saved as format in Problem Set 2 CS229
	 */
	private void outputSamples() {
		File outf = new File("files/" + outputFileName);
		try {
			Writer writer = new BufferedWriter(
					new OutputStreamWriter(new FileOutputStream(outf), "UTF-8"));
			StringBuffer sb = new StringBuffer();
			for (Relation relation : relations) {
				// append label
				if (relation.getLabel()) {
					numPositive++;
					sb.append("1 ");
				} else {
					numNegative++;
					sb.append("0 ");
				}
				// append features
				sb.append(transformFeatureVector(relation.getFeaturesVector()));
				// append end flag -1
				sb.append("-1\n");
			}
//			sb.insert(0, printTokenList());
			sb.insert(0, relations.size() + " " + maxColIndex + "\n");
			sb.insert(0, "FEATURE_TRAIN_MATRIX\n");
			writer.write(sb.toString());
			System.out.println("max column index: " + maxColIndex);
			System.out.println("num of positive relations: " + numPositive);
			System.out.println("num of negative relations: " + numNegative);
			System.out.println("feature index table: ");
			System.out.print(printFeatureIndex());
			writer.close();
			/* write pos sequence dictionary to file */
//			writePOSSequenceToFile();
		} catch (IOException e1) {
				e1.printStackTrace();
		}
	}
	
	/**
	 * This function is used to change format of feature vectors in format
	 * that is convenient for ML module. Can be customized to any format.
	 * @param rawFeatureVector
	 * @return
	 */
	private String transformFeatureVector(String rawFeatureVector) {
		StringBuffer sb = new StringBuffer();
		Integer prevIndex = 0;
		String[] pairs = rawFeatureVector.split(" ");
		for (int i = 0; i < pairs.length; i++) {
			String[] tokens = pairs[i].split(":");
			Integer featureIndex = Integer.parseInt(tokens[0]);
			Integer frequency = Integer.parseInt(tokens[1]);
			sb.append((featureIndex - prevIndex) + " " + frequency + " ");
			prevIndex = featureIndex;
			// dynamically change the max column based on feature index of pos sequence
			if ((i == pairs.length - 1) && featureIndex > maxColIndex)
				maxColIndex = featureIndex;
		}
		return sb.toString();
	}
	
	public static void main(String[] args) {
		if (args.length != 2) {
			System.err.println("usage: Processor inputfilename outputfilename");
			return;
		} else {
			Processor processor = new Processor(args[0], args[1]);
			File inf = processor.readFile(processor.inputFileName);
			processor.parseJSONFile(inf);
			processor.readDictionaries();
			processor.extractRelations();
			if (enableExport) {
				processor.exportToDB();
			} else {
				processor.outputSamples();
			}				
		}
	}
	
	/**********************************************
	 * Set of utility functions for debugging
	 ***********************************************/
	private String printTokenList() {
		StringBuffer sb = new StringBuffer();
		sb.append("NumWords ");
		sb.append("NumStopWords ");
		sb.append("NumCapWords ");
		sb.append("NumPuncs ");
		sb.append("NumNPsBtw ");
		for(String ner : nerDictionary.keySet()) {
			sb.append("NERE1" + ner + " ");
		}
		for(String ner : nerDictionary.keySet()) {
			sb.append("NERE2" + ner + " ");
		}
		for(String ner : POSDictionary.keySet()) {
			sb.append("POSE1" + ner + " ");
		}
		for(String ner : POSDictionary.keySet()) {
			sb.append("POSE2" + ner + " ");
		}
		sb.append("\n");
		return sb.toString();
	}
	
	/* A little bit hack here, output updated POSSequenceDictionary here,
	 * construct pos sequence dictionary along the way of building maps 
	 * sort pos sequence dictionary first, then output to file for use in next round*/
	@SuppressWarnings("unchecked")
	private void writePOSSequenceToFile() {
		File outf = new File("files/" + POS_SEQUENCE_FILE_NAME);
		try {
			Writer writer = new BufferedWriter(
					new OutputStreamWriter(new FileOutputStream(outf), "UTF-8"));
			StringBuffer sb = new StringBuffer();
			POSSequenceDictonary = (HashMap<String, Integer>)sortByComparator(POSSequenceDictonary);
			for (String key : POSSequenceDictonary.keySet()) {
				sb.append(key + "\n");
			}
			writer.write(sb.toString());
			writer.close();
		} catch (IOException e1) {
				e1.printStackTrace();
		}
	}
	
	/* construct predicated label data structure */
	private ArrayList<Integer> constructPredictedLabel(String fileName) {
		ArrayList<Integer> predicatedLabels = new ArrayList<Integer>();
		File inf = Processor.readFile(fileName);
		try {
			BufferedReader reader;
			reader = new BufferedReader(new FileReader(inf));
			String line = null;
			while((line = reader.readLine()) != null ) {
				if (line.equals("   1.0000000e+00"))
					predicatedLabels.add(1);
				else
					predicatedLabels.add(0);
			}
			reader.close();
			return predicatedLabels;
		} catch (FileNotFoundException e) {
			e.printStackTrace();
			return null;
		} catch (IOException e1) {
			e1.printStackTrace();
			return null;
		}
	}
	
	private static Map sortByComparator(Map unsortMap) {
		 
		List list = new LinkedList(unsortMap.entrySet());
 
		// sort list based on comparator
		Collections.sort(list, new Comparator() {
			public int compare(Object o1, Object o2) {
				return ((Comparable) ((Map.Entry) (o1)).getValue())
                                       .compareTo(((Map.Entry) (o2)).getValue());
			}
		});
 
		// put sorted list into map again
                //LinkedHashMap make sure order in which keys were inserted
		Map sortedMap = new LinkedHashMap();
		for (Iterator it = list.iterator(); it.hasNext();) {
			Map.Entry entry = (Map.Entry) it.next();
			sortedMap.put(entry.getKey(), entry.getValue());
		}
		return sortedMap;
	}
	
	private void printArticles() {
		for (Article article : articles) {
			System.out.println(article.toString());
		}
	}
	
	private void printRelations() {
		for (Relation relation : relations) {
			System.out.println(relation.toString());
		}
	}
}
